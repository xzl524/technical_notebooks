{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 监督学习\n",
    "\n",
    "#### 主要内容\n",
    "* 1 监督学习综述\n",
    "    * 1.1 回归问题\n",
    "    * 1.2 分类问题\n",
    "    * 1.3 方差与偏差\n",
    "    * 1.4 模型评估与选择\n",
    "    * 1.5 实际经验总结\n",
    "* 2 监督学习算法\n",
    "    * 2.1 线性回归\n",
    "    * 2.2 逻辑回归\n",
    "    * 2.3 神经网络\n",
    "    * 2.4 决策树\n",
    "    * 2.5 Boosting\n",
    "    * 2.6 随机森林\n",
    "    * 2.7 贝叶斯方法\n",
    "    * 2.8 支持向量机\n",
    "* 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 监督学习综述\n",
    "\n",
    "#### 1.1 回归问题\n",
    "\n",
    "数学表达\n",
    "\n",
    "* 利用数据估计$E(Y|x)$\n",
    "\n",
    "估计方法\n",
    "* K近邻回归\n",
    "$$E(Y|x)\\approx \\frac{1}{K}\\sum_{k=1}^K (y_k|x_k\\in N_K(x))$$\n",
    "\n",
    "* 最小二乘法\n",
    "\n",
    "    \\begin{align}\n",
    "    E(Y|x)\n",
    "    &=\\underset{f}{\\arg\\min} E[Y-f(x)]^2\\\\\n",
    "    &=\\underset{f}{\\arg\\min} E\\{\\frac{1}{N}\\sum_{i=1}^N [y_i-f(x_i)]^2\\}\\\\\n",
    "    &\\approx \\underset{f}{\\arg\\min} \\frac{1}{N}\\sum_{i=1}^N [y_i-f(x_i)]^2\\\\\n",
    "    &=\\underset{f}{\\arg\\min} \\sum_{i=1}^N [y_i-f(x_i)]^2\n",
    "    \\end{align}\n",
    "\n",
    "    这一方法又叫做最小化经验平方损失估计：$l(y,f)=(y-f)^2$。\n",
    "\n",
    "* 极大似然估计\n",
    "\n",
    "    若$Y=E(Y|x)+\\varepsilon$，且$\\varepsilon\\sim N(0,\\sigma^2)$，即$Y\\sim N(E(Y|x),\\sigma^2)$，则\n",
    "\n",
    "    \\begin{align}\n",
    "    E(Y|x)\n",
    "    &\\approx \\underset{E(Y|x)}{\\arg\\max}\\ln L(y_1,y_2,...,y_N)\\\\\n",
    "    &=\\underset{E(Y|x)}{\\arg\\max}\\ln \\prod_{i=1}^N P(y_i)\\\\\n",
    "    &=\\underset{E(Y|x)}{\\arg\\max}\\ln \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma}e^-\\frac{[y_i-E(y_i|x_i)]^2}{2\\sigma^2}\\\\\n",
    "    &=\\underset{f}{\\arg\\max}\\ln \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma}e^-\\frac{[y_i-f(x_i)]^2}{2\\sigma^2}\\\\\n",
    "    &=\\underset{f}{\\arg\\max}\\sum_{i=1}^N-\\frac{[y_i-f(x_i)]^2}{2\\sigma^2}+C\\\\\n",
    "    &=\\underset{f}{\\arg\\max}\\sum_{i=1}^N-[y_i-f(x_i)]^2\\\\\n",
    "    &=\\underset{f}{\\arg\\min}\\sum_{i=1}^N[y_i-f(x_i)]^2\\\\\n",
    "    \\end{align}\n",
    "\n",
    "    所以此时极大似然估计与最小二乘法等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 分类问题\n",
    "\n",
    "数学表达\n",
    "* 利用数据估计类别条件概率$P⁡(Y=g|x), g=1,...,G$\n",
    "\n",
    "简化分析\n",
    "* 分类问题的讨论可仅限于二元分类问题。多类别分类问题可转化为二元分类问题，针对每个二元分类问题求出该类别的类别条件概率$P⁡(Y=1|x)$。若有G个类别，则只需依此方法求出G-1个类别条件概率，剩下一个类别条件概率根据概率总和为1计算得出。\n",
    "\n",
    "估计方法\n",
    "* K近邻分类\n",
    "\n",
    "$$P(Y=1|x)\\approx \\frac{1}{K}\\sum_{k=1}^K I(y_k=1|x_k\\in N_K(x))$$\n",
    "\n",
    "* 极大似然估计\n",
    "    * $Y\\in\\{0,1\\}$\n",
    "    \n",
    "    \\begin{align}\n",
    "    P(Y=1|x)\n",
    "    &\\approx \\underset{P}{\\arg\\max}\\ln L(y_1,y_2,...,y_n)\\\\\n",
    "    &=\\underset{P}{\\arg\\max}\\ln \\prod_{i=1}^n P(y_i=1|x_i)^{y_i}[1-P(y_i=1|x_i)]^{1-y_i}\\\\\n",
    "    &=\\underset{P}{\\arg\\max}\\sum_{i=1}^n\\{y_i\\ln P(y_i=1|x_i)+(1-y_i)\\ln [1-P(y_i=1|x_i)]\\}\\\\\n",
    "    &=\\underset{P(f)=f}{\\arg\\max}\\sum_{i=1}^n\\{y_i\\ln f(x_i)+(1-y_i)\\ln [1-f(x_i)]\\}\\\\\n",
    "    &=\\underset{P(f)=f}{\\arg\\min}\\sum_{i=1}^n-\\{y_i\\ln f(x_i)+(1-y_i)\\ln [1-f(x_i)]\\}\\\\\n",
    "    \\end{align}\n",
    "\n",
    "    所以此时等价于最小化经验对数损失估计：$l(y,f)=-[y\\ln f+(1-y)\\ln⁡(1-f)]$。\n",
    "    \n",
    "    * $Y\\in\\{-1,1\\}$\n",
    "\n",
    "    \\begin{align}\n",
    "    P(Y=1|x)\n",
    "    &\\approx \\underset{P}{\\arg\\max}\\ln L(y_1,y_2,...,y_n)\\\\\n",
    "    &=\\underset{P}{\\arg\\max}\\ln \\prod_{i=1}^n P(y_i=1|x_i)^{\\frac{y_i+1}{2}}[1-P(y_i=1|x_i)]^{\\frac{1-y_i}{2}}\\\\\n",
    "    &=\\underset{P}{\\arg\\max}\\sum_{i=1}^n\\{\\frac{y_i+1}{2}\\ln P(y_i=1|x_i)+\\frac{1-y_i}{2}\\ln [1-P(y_i=1|x_i)]\\}\\\\\n",
    "    &=\\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\max}\\sum_{i=1}^n\\{\\frac{y_i+1}{2}\\ln \\frac{e^{2f(x_i)}}{1+e^{2f(x_i)}}+\\frac{1-y_i}{2}\\ln \\frac{1}{1+e^{2f(x_i)}}\\}\\\\\n",
    "    &=\\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\min}\\sum_{i=1}^n\\ln\\frac{e^{(y_i-1)f(x_i)}+e^{(y_i+1)f(x_i)}}{e^{2y_if(x_i)}}\\\\\n",
    "    &=\\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\min}\\sum_{i=1}^n\\ln\\frac{e^{2y_if(x_i)}+1}{e^{2y_if(x_i)}}\\\\\n",
    "    &=\\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\min}\\sum_{i=1}^n\\ln (1+e^{-2y_if(x_i)})\\\\\n",
    "    \\end{align}\n",
    "\n",
    "    所以此时等价于最小化经验deviance估计：$l(y,f)=\\ln⁡(1+e^{-2yf})$。\n",
    "\n",
    "* 最小化经验指数损失估计：$l(y,f)=e^{-yf}, y\\in\\{-1,1\\}$\n",
    "    * 由最小化经验deviance估计的讨论可得：\n",
    "    \\begin{align}\n",
    "    P(Y=1|x)\n",
    "    &\\approx \\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\min}\\sum_{i=1}^n\\ln (1+e^{-2y_if(x_i)})\\\\\n",
    "    &\\approx \\underset{P(f)=\\frac{e^{2f}}{1+e^{2f}}}{\\arg\\min}\\sum_{i=1}^ne^{-y_if(x_i)}\n",
    "    \\end{align}\n",
    "\n",
    "    * 指数损失和deviance的区别：当间隔$yf$为很大的负数时，指数损失比deviance要大。这意味着，当样本数据中有异常值时（例如类别标记错误），使用指数损失会格外在意这些异常值，而deviance相对来说不容易受到异常值的干扰。因此使用指数损失的算法不如使用deviance的算法性能稳定。\n",
    "\n",
    "类别判定\n",
    "* 得到类别条件概率$P(Y=g|x)$的估计后，通过最小化类别判定损失的期望来确定预测类别。通常选择类别判定损失为0-1损失：$l(y,f)=I(y\\neq f)$，则：\n",
    "\n",
    "    \\begin{align}\n",
    "    \\hat g\n",
    "    &=\\underset{f}{\\arg\\min}E[I(y\\neq f)]\\\\\n",
    "    &=\\underset{f}{\\arg\\min}\\sum_{g=1}^G P(y=g)I(g\\neq f)\\\\\n",
    "    &=\\underset{f}{\\arg\\min}\\sum_{g=1}^G P(y=g)[I(g\\neq f)+I(g=f)-I(g=f)]\\\\\n",
    "    &=\\underset{f}{\\arg\\min}\\sum_{g=1}^G P(y=g)[1-I(g=f)]\\\\\n",
    "    &=\\underset{f}{\\arg\\min} [1-\\sum_{g=1}^GP(y=g)I(g=f)]\\\\\n",
    "    &=\\underset{f}{\\arg\\min} [1-P(y=f)]\\\\\n",
    "    &=\\underset{f}{\\arg\\max} P(y=f)\\\\\n",
    "    \\end{align}\n",
    "\n",
    "    即以类别条件概率最大的类别作为预测类别：$\\hat g(x)=\\underset{g}{\\arg\\max} P⁡(Y=g|x), g=1,...,G$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 方差与偏差\n",
    "\n",
    "选取平方损失，则估计值$\\hat f(x)$与真值$Y$之间的期望误差为：\n",
    "\n",
    "\\begin{align}\n",
    "EPE\n",
    "&=E[Y-\\hat f(x)]^2\\\\\n",
    "&=E[Y-E(Y|x)+E(Y|x)-\\hat f(x)]^2\\\\\n",
    "&=E[Y-E(Y|x)]^2+E[E(Y|x)-\\hat f(x)]^2+2E[Y-E(Y|x)]E[E(Y|x)-\\hat f(x)]（假设两者独立）\\\\\n",
    "&=E[Y-E(Y|x)]^2+E[E(Y|x)-\\hat f(x)]^2\\\\\n",
    "&=DY+E[E(Y|x)-E(\\hat f(x))+E(\\hat f(x))-\\hat f(x)]^2\\\\\n",
    "&=DY+E[E(Y|x)-E(\\hat f(x))]^2+E[E(\\hat f(x))-\\hat f(x)]^2+2E[E(Y|x)-E(\\hat f(x))](E(\\hat f(x)-E\\hat f(x))（假设两者独立）\\\\\n",
    "&=DY+E[E(Y|x)-E(\\hat f(x))]^2+E[E(\\hat f(x))-\\hat f(x)]^2\\\\\n",
    "&=DY+E[E(Y|x)-E(\\hat f(x))]^2+D\\hat f(x)\n",
    "\\end{align}\n",
    "\n",
    "偏差：$E[E(Y|x)-E(\\hat f(x))]^2$\n",
    "* 度量模型$\\hat f(x)$本身的拟合能力所造成的估计误差。\n",
    "* 原因：模型$\\hat f(x)$复杂度过低，或者模型的假设不正确。此时模型无法充分学习数据中的规律，表现为欠拟合。\n",
    "\n",
    "方差：$D\\hat f(x)$\n",
    "* 度量数据扰动所造成的估计误差。\n",
    "* 原因：模型$\\hat f(x)$复杂度过高，或者数据量过小。此时模型试图学习数据中的所有内容，而数据中往往存在噪音，因此这时模型学习到的不是真正的规律，而是噪音。表现为过拟合。\n",
    "\n",
    "噪音：$DY$\n",
    "* 度量学习问题本身的难度。\n",
    "\n",
    "当模型复杂度较低时，方差很小但偏差很大；当模型复杂度较高时，偏差很小但方差很大。监督学习的关键就在于需要选择能够使得方差与偏差之和最小的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 模型评估和选择\n",
    "\n",
    "理论误差\n",
    "\n",
    "* 预测误差（测试误差或者泛化误差）：$Err_T=E[l(Y,\\hat f(x))|T]$，$T$为训练集\n",
    "* 预测误差期望：$Err=E[l(Y,\\hat f(x))]=E[Err_T]$\n",
    "\n",
    "为了进行模型选择和评估，目标是对预测误差进行估计，但是对于统计分析来说估计预测误差期望更合适也往往更容易实现。\n",
    "\n",
    "估计方法\n",
    "* 模型选择\n",
    "    * 对不同模型的误差进行估计，从而选择最好的一个：\n",
    "        * 数据量较大时，在验证集上估计预测误差。\n",
    "        * 数据量较小时，使用交叉验证（cross validation）法或自助法（bootstrap）来估计预测误差期望。\n",
    "\n",
    "* 模型评估\n",
    "    * 在测试集（新数据）上估计最终模型的预测误差。\n",
    "\n",
    "分类问题评估指标\n",
    "* 预测错误率（=1-预测精度）\n",
    "* 平均对数损失\n",
    "* 查准率、查全率与F1数\n",
    "    * 查准率（准确率）P：真正例在所有预测正例中所占的比例。\n",
    "    * 查全率（召回率）R：真正例在所有正例中所占的比例。\n",
    "    * F1数：查全率与查准率的调和平均：$\\frac{1}{F}=\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})$，若查全率和查准率的重要程度相当时，可以使用F1数作为评价模型性能的综合指标。F1数越大，模型越好。\n",
    "* ROC曲线与AUC\n",
    "    * ROC曲线：用于评估分类器的分类性能。ROC曲线图的横轴为假正例率（$FPR=\\frac{FP}{TN+FP}$），纵轴为真正例率（$TPR=\\frac{TP}{TP+FN}$）。对于某一个分类器，当分类阈值从0到1变化时，其对应的假正例率和真正例率也会相应变化，从而在ROC图上连成一条曲线。比如当阈值为0时，所有的样本都会被预测为正类，因此假正例率和真正例率都为1，对应ROC图上的(1,1)。当阈值为1时，所有的样本都会被预测为反类，因此假正例率和真正例率都为0，对应ROC图上的(0,0)。\n",
    "    * AUC：ROC曲线下的面积，面积越大可认为模型性能越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 实际经验总结\n",
    "\n",
    "#### 训练和调试模型\n",
    "\n",
    "训练模型\n",
    "\n",
    "* 选择恰当的性能度量标准。对于分类问题，若数据类别不平衡，可使用F1或者AUC\n",
    "* 建立基准模型\n",
    "* 快速建立一个简单的模型并完成测试，完成全部训练步骤\n",
    "* 确定足够大的验证集以便反映不同模型的性能差异\n",
    "* 确定足够大的测试集以便更准确地评估模型性能\n",
    "\n",
    "调试模型\n",
    "\n",
    "* 利用人类水平与训练集误差的差异调试模型的偏差\n",
    "* 利用训练集误差与验证集误差的差异（训练集和验证集同分布）调试模型的方差\n",
    "    \n",
    "其他注意事项\n",
    "\n",
    "* 训练过程不能泄露到测试数据\n",
    "* 交叉验证用来选择超参数，然后使用测试集和验证集共同完成训练\n",
    "* 若使用梯度下降法，需要对输入数据标准化以改善训练效果\n",
    "\n",
    "#### 改进数据品质\n",
    "\n",
    "* 模型过拟合时，搜集更多的数据\n",
    "* 验证集和测试集需要反映未来真正的应用场景会遇到的数据（同分布）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 监督学习算法\n",
    "\n",
    "#### 2.1 线性回归（Linear Regression）\n",
    "\n",
    "#### 最小二乘法\n",
    "\n",
    "线性回归模型的表达式为：\n",
    "\n",
    "$$f(\\textbf x)=\\beta_0+\\sum_{j=1}^px^{j}\\beta_{j}=\\textbf x^T\\beta$$\n",
    "\n",
    "当$\\textbf X$列向量满秩时，最小二乘法的解析解为：\n",
    "\n",
    "$$\\hat\\beta=(\\textbf X^T\\textbf X)^{-1}\\textbf X^Ty$$\n",
    "\n",
    "证明：\n",
    "\n",
    "$$RSS(\\beta)=\\sum_{i=1}^N(y_i-x_i^T\\beta)^2=(\\textbf y-\\textbf X\\beta)^T(\\textbf y-\\textbf X\\beta)$$\n",
    "\n",
    "要使得残差和$RSS(\\beta)$最小，就需要对其求导，并找到导数为0的条件。\n",
    "\n",
    "首先引入4个线性代数公式：\n",
    "\n",
    "* $trAB=trBA$\n",
    "* $\\bigtriangledown_AtrAB=B^T $\n",
    "* $\\bigtriangledown_{A^T}f(A)=(\\bigtriangledown_Af(A))^T$\n",
    "* $\\bigtriangledown_AtrABA^TC=CAB+C^TAB^T$\n",
    "\n",
    "对残差和求导可得：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bigtriangledown_\\beta RSS(\\beta)\n",
    "&=\\bigtriangledown_\\beta (y-X\\beta)^T(y-X\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta (y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta tr(y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta [tr(y^Ty)-2tr(y^TX\\beta)+tr(\\beta^TX^TX\\beta)]\\\\\n",
    "&=\\bigtriangledown_\\beta tr(\\beta^TX^TX\\beta)-2\\bigtriangledown_\\beta tr(\\beta y^TX)\\\\\n",
    "&=[\\bigtriangledown_{\\beta^T} tr(\\beta^TX^TX\\beta)]^T-2X^Ty\\\\\n",
    "&=[\\beta^TX^TX+\\beta^TX^TX]^T-2X^Ty\\\\\n",
    "&=2X^TX\\beta-2X^Ty\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "当$\\bigtriangledown_\\beta RSS(\\beta)=0$时有：$X^TX\\beta=X^Ty$。\n",
    "\n",
    "下面证明当$X$列向量满秩时，$X^TX$可逆。\n",
    "\n",
    "$X\\in R^{n\\times {(p+1)}}$，当$X$列向量满秩时，$r(X)=p+1$。\n",
    "\n",
    "则此时$k_0X_0+k_1X_1+k_2X_2+...+k_pX_p=0$没有非零解，即对任意非零向量$a=[k_0,k_1,k_2,...,k_p]^T$，都有$Xa\\neq 0$。\n",
    "\n",
    "则内积$(Xa,Xa)=(Xa)^T(Xa)=a^TX^TXa>0$。\n",
    "\n",
    "又$(X^TX)^T=X^TX$，即$X^TX$为对称矩阵，因此$X^TX$为正定矩阵，也就是说$X^TX$可逆，所以就有：\n",
    "\n",
    "$$\\hat\\beta=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "又$\\frac{\\partial^2RSS}{\\partial\\beta\\partial\\beta^T}=2X^TX>0$，因此当一阶导数为零时求得的极值确实为最小值。\n",
    "\n",
    "讨论\n",
    "\n",
    "有两种情况会导致$X$列向量不满秩：\n",
    "* 特征冗余（特征之间线性相关）\n",
    "* 特征过多（特征数多于样本数）\n",
    "\n",
    "若$X$列向量不满秩，则无法得到解析解，此时可使用梯度下降法。另外，即使$X$只是接近于不是满秩矩阵，此时解析解对$\\beta$所做的估计也会有较大的方差。遇到这种情况可以使用正则化（regularization）估计，其实质为有偏估计。有偏估计与解析解的无偏估计相比，偏差会更大，但方差可能会更小。\n",
    "\n",
    "#### 岭回归（Ridge Regression）\n",
    "$$E(Y|x)\\approx \\underset{f(\\beta)}{\\arg\\min}\\{\\sum_{i=1}^N(y_i-x_i^T\\beta)^2+\\lambda\\sum_{j=1}^p \\beta_j^2\\}$$\n",
    "\n",
    "* 注意：正则项中不包含$\\beta_0$\n",
    "* 岭回归的解析解为：$\\hat\\beta^{ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
    "\n",
    "#### LASSO\n",
    "$$E(Y|x)\\approx \\underset{f(\\beta)}{\\arg\\min}\\{\\sum_{i=1}^N(y_i-x_i^T\\beta)^2+\\lambda\\sum_{j=1}^p |\\beta_j|\\}$$\n",
    "* 注意：正则项中不包含$\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 逻辑回归（Logistic Regression）\n",
    "\n",
    "逻辑回归模型的表达式为：\n",
    "\n",
    "$$p(Y=1|x)=\\frac{1}{1+e^{-\\beta^Tx}}$$\n",
    "\n",
    "使用对数损失进行估计：\n",
    "\n",
    "$$\\hat \\beta=\\underset{\\beta}{\\arg\\min}\\sum_{i=1}^n-\\{y_i\\ln \\frac{1}{1+e^{-\\beta^Tx_i}}+(1-y_i)\\ln [1-\\frac{1}{1+e^{-\\beta^Tx_i}}]\\}$$\n",
    "\n",
    "可加入正则项进行估计，以降低模型方差：\n",
    "\n",
    "$$\\hat \\beta=\\underset{\\beta}{\\arg\\min}\\sum_{i=1}^n\\{-y_i\\ln \\frac{1}{1+e^{-\\beta^Tx_i}}-(1-y_i)\\ln [1-\\frac{1}{1+e^{-\\beta^Tx_i}}]+\\lambda\\sum_{j=1}^p \\beta_j^2\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 神经网络（Neural Network）\n",
    "\n",
    "#### 2.3.1 基本结构\n",
    "\n",
    "神经网络：非线性统计模型，其非线性特性来源于模型中激活函数的非线性。若激活函数变为线性函数，那么整个神经网络就退化成线性模型。\n",
    "\n",
    "神经网络包括三种数据层：输入层、隐藏层和输出层。神经网络的层数为隐藏层层数+1（输出层）。深度神经网络（Deep Neural Network）指隐藏层多于一个的神经网络。\n",
    "\n",
    "隐藏层层数一般由实验和应用背景来确定。总的来说，深度神经网络比浅层神经网络要好。一个直观的解释是：为构建一个具有深度神经网络性能的浅层神经网络，其需要的隐藏单元数相对于深度神经网络来说需要呈指数级增加。\n",
    "\n",
    "#### 激活函数（activation function）\n",
    "\n",
    "隐藏层常用的非线性激活函数有3种：\n",
    "\n",
    "1. ReLU（修正线性单元，Rectified Linear Unit）函数：$a(z)=\\max⁡(0,z)$，当前默认使用的激活函数。\n",
    "2. Sigmoid函数：$a(z)=1/(1+e^{-z})$，现在只用于一种情形：二元分类问题的输出层。\n",
    "3. Tanh函数：$a(z)=(e^z-e^{-z})/(e^z+e^{-z})$。\n",
    "\n",
    "Sigmoid和Tanh函数共同的一个问题是：当$z$很大或者很小时，激活函数的梯度非常小（梯度消失），这会导致梯度下降法的学习速度变得非常慢。\n",
    "\n",
    "输出层的激活函数一般有两种：\n",
    "* 对于回归问题，输出层的激活函数为恒等函数：$a(z)=z$。\n",
    "* 对于多分类问题，输出层的激活函数为softmax函数（Sigmoid函数的一般形式）：$a(z_k)=e^{z_k}/\\sum_{i=1}^Ke^{z_i}$。\n",
    "\n",
    "#### 2.3.2 训练神经网络\n",
    "\n",
    "#### 误差逆传播算法\n",
    "\n",
    "训练神经网络模型是通过梯度下降法来最小化经验损失，这种方法也叫做逆传播（Back-Propagation，BP）。具体算法如下：\n",
    "\n",
    "1. 前向传播：利用当前模型参数$\\boldsymbol\\omega$计算模型的预测值，然后计算经验损失$J$。\n",
    "2. 逆传播：计算当前经验损失$J$对于模型参数的梯度：$-\\partial J(\\boldsymbol\\omega)/\\partial\\boldsymbol\\omega$\n",
    "3. 更新参数：$\\boldsymbol\\omega≔\\boldsymbol\\omega-\\alpha\\cdot\\partial J(\\boldsymbol\\omega)/\\partial\\boldsymbol\\omega$。\n",
    "\n",
    "梯度为向量，定义了对经验损失进行优化的方向。在更新参数时，最重要的不是梯度的大小，而是梯度的方向，即对各个参数$\\omega_i$更新的比例。$\\alpha$为学习率，是无量纲常数。学习率可以成比例地改变所有参数每一步更新的大小。\n",
    "\n",
    "#### 2.3.3 与机器学习相关的问题\n",
    "\n",
    "#### 处理过拟合的方法\n",
    "\n",
    "* 正则化（Regularization）\n",
    "* Dropout正则化：完全关闭部分隐藏单元。注意：在测试模型性能时不能使用dropout。\n",
    "\n",
    "其他方法：数据增强（data augmentation），早停法（early stopping）。\n",
    "\n",
    "#### 迷你批次（mini-batch）学习\n",
    "\n",
    "由对参数进行一次更新所使用样本数量$s$（训练样本总数量为$m$）的不同把神经网络的训练分为3种类型：\n",
    "\n",
    "1. $s=m$，全批次梯度下降，适用于训练集很小（$m<=2000$）的学习；\n",
    "2. $s=1$，随机（stochastic）梯度下降，适用于在线学习；\n",
    "3. $1<s<m$：迷你批次梯度下降，适用于训练集数据超过CPU/GPU内存的学习。常用的迷你批次大小$s$为：64，128，256和512。迷你批次大小需要与CPU/GPU的内存相匹配。\n",
    "\n",
    "“一代”（epoch）：使用迷你批次学习时，将所有训练数据使用一遍称为“一代”。\n",
    "\n",
    "#### 可调参数\n",
    "\n",
    "1. 学习率\n",
    "2. 隐藏层层数\n",
    "3. 隐藏单元个数\n",
    "4. dropout值/正则化参数\n",
    "5. 迷你批次大小\n",
    "\n",
    "#### 其他\n",
    "\n",
    "若没有测试集而只有验证集可能也是可以的。\n",
    "\n",
    "#### 2.3.4 解决神经网络模型自身问题的技巧\n",
    "\n",
    "#### 加快训练速度的方法\n",
    "\n",
    "* 对输入数据进行归一化，这使得所有参数可以以相同量级的速度进行更新。如果没有对输入数据进行归一化，学习率需要设定得非常小，否则优化可能会发散。\n",
    "* 使用Adam算法，修正梯度下降的方向。Adam = Momentum + RMSprop。\n",
    "* 缓慢降低学习率。\n",
    "\n",
    "#### 参数初始化（Weight Initialization）\n",
    "\n",
    "模型参数需要在0附近进行随机初始化（Random Initialization），而参数中的偏差$b$的初始值可以为0。模型参数不能初始化为相同的值（比如0或者1），否则同一层所有隐藏单元的参数更新会一模一样，这就导致了无论使用多少个隐藏单元都会退化为一个隐藏单元。参数也不宜太大，否则会导致sigmoid和tanh激活函数的梯度很小，降低学习速度。当然，若使用ReLU激活函数这一问题没有那么严重。\n",
    "\n",
    "#### 梯度消失和梯度爆炸（Vanishing and Exploding Gradients）\n",
    "\n",
    "深度神经网络具有梯度消失和梯度爆炸的问题。这一问题的根本原因是：靠前层的梯度是其后所有层的乘积，后面层的梯度值会以乘积的形式放大到靠前层，导致靠前层的梯度要么过小（梯度消失），要么过大（梯度爆炸）。\n",
    "\n",
    "解决梯度消失的方法有：使用ReLU激活函数, RNN中使用LSTM，以及Batch Normalization。\n",
    "\n",
    "另外，恰当的模型初始化也可以缓解梯度消失和梯度爆炸的问题（源于Andrew Ng的深度学习课程）。恰当的模型初始化是指控制依据前一层输入单元的个数，调整参数初始值的范围：\n",
    "\n",
    "$$\\omega^{[L]}=np.random.randn(shape)\\times np.sqrt(\\frac{2}{n^{[L-1]}})$$\n",
    "\n",
    "这一方法推荐配合ReLU激活函数使用。\n",
    "\n",
    "#### 批标准化（Batch Normalization）\n",
    "\n",
    "* 类似于对输入进行标准化，可以加快训练速度（解决梯度消失的问题）；\n",
    "* 还有一点正则化的效果；\n",
    "* 在激活函数之前而不是之后对输入$z$进行批标准化（Andrew Ng的深度学习课程推荐方法）；\n",
    "* 在使用迷你批次学习时，批标准化需要被用于单个迷你批次上。\n",
    "\n",
    "#### 迁移学习（Transfer Learning）\n",
    "\n",
    "将从任务A学习到的模型迁移到任务B，对任务B的数据进行学习。\n",
    "\n",
    "适用迁移学习的情形有：\n",
    "\n",
    "* 任务A和任务B有相同的输入；\n",
    "* 任务A的数据比任务B的数据多得多；\n",
    "* 从任务A中学习到的底层特征对任务B有帮助。\n",
    "\n",
    "#### 2.3.5 卷积神经网络（Convolutional Neural Network）\n",
    "\n",
    "为什么做卷积？\n",
    "* 参数共享：使用一个特征识别器识别图片不同部位的相同特征。\n",
    "* 关联的稀疏性：输出往往只与输入的很少一部分特征相关联，卷积可以把这些输入特征提取出来。\n",
    "\n",
    "两类特殊的数据层：\n",
    "1. 卷积层（convolutional layer）\n",
    "\n",
    "    过滤器（filter）\n",
    "        * 训练的目的就是学到过滤器的参数\n",
    "        * 过滤器深度与输入数据的深度保持一致\n",
    "        * 同一卷积层中可以有多个过滤器\n",
    "\n",
    "    步长（stride）\n",
    "        * 步长为过滤器每一步的移动距离\n",
    "\n",
    "    填充（padding）\n",
    "        * 相等填充（same padding），保持输入和输出的大小一致\n",
    "        * 合理填充（valid padding）/0填充（zero padding）：不进行填充\n",
    "\n",
    "    $$O=\\frac{W-K+2P}{S}+1$$\n",
    "\n",
    "    $O$：输出高度/长度，$W$：输入高度/长度，$K$：过滤器大小，$P$：填充大小，$S$：步长\n",
    "2. 池化层（pooling layer）\n",
    "    * 池化层过滤器的步长与其大小相同\n",
    "    * 最大池化（maxpooling）和平均池化（average pooling）\n",
    "    \n",
    "#### 2.3.6 循环神经网络（Recurrent Neural Network）\n",
    "\n",
    "模型输入\n",
    "* 序列数据（Sequence Data）或叫做序列向量（Sequence of Vectors），例如一段文字、语言等。\n",
    "\n",
    "模型输出\n",
    "* 模型输出不仅与序列向量当前的输入有关，也与序列向量当前输入之前所有的输入有关（有时也与当前输入之后的输入有关）。\n",
    "\n",
    "激活函数\n",
    "* Tanh/ReLU函数\n",
    "\n",
    "处理梯度消失\n",
    "* 使用GRU模型和LSTM模型\n",
    "\n",
    "RNN模型\n",
    "* GRU模型（Gated Recurrent Unit）\n",
    "* LSTM模型（Long-short Term Memory，GRU的一般版本）\n",
    "* 双向RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 决策树（Decision Tree）\n",
    "\n",
    "* 采取“分而治之”策略，把特征空间划分成一个个长方形的区域，然后对每一个区域内的数据拟合一个简单的模型（例如常数）\n",
    "* 树模型指二叉树模型\n",
    "\n",
    "#### 2.4.1 回归树\n",
    "\n",
    "回归树模型的表达式为：\n",
    "\n",
    "$$f(x)=\\sum_{m=1}^Mc_m I(x\\in R_m)$$\n",
    "$$\\hat c_m=ave(y_i|x_i\\in R_m)$$\n",
    "\n",
    "求解过程：\n",
    "\n",
    "1. 确定最佳划分变量$j$和最佳划分点$s$：\n",
    "\n",
    "    $$\\min_{j,s}⁡[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 +\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "\n",
    "    其中：\n",
    "\n",
    "    $$R_1(j,s)=\\{X|X_j\\leqslant s\\}, R_2(j,s)=\\{X|X_j>s\\}$$\n",
    "    $$\\hat c_1=ave(y_i│x_i\\in R_1(j,s)), \\hat c_2=ave(y_i│x_i\\in R_2(j,s))$$\n",
    "\n",
    "2. 使用变量$j$和划分点$s$进行划分后，在每个区域内重复上述过程，直到回归树的叶节点大小都小于或等于设定值，此时形成的树记做$T_0$。\n",
    "\n",
    "3. 通过最小化成本复杂度对$T_0$进行剪枝，降低树的复杂度，防止过拟合。\n",
    "\n",
    "    成本复杂度：$C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha|T|$，其中：\n",
    "    * $|T|$：叶节点个数\n",
    "    * $N_m=\\#\\{x_i\\in R_m\\}$\n",
    "    * $\\hat c_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m}y_i$\n",
    "    * $Q_m(T)=\\frac{1}{N_m}\\sum_{x_i\\in R_m}(y_i-\\hat c_m)^2$\n",
    "\n",
    "#### 2.4.2 分类树\n",
    "\n",
    "分类树与回归树相比，唯一不同的是划分节点和进行剪枝所使用的节点杂度$Q_m(T)$的度量方法：\n",
    "\n",
    "* 误分率：$\\frac{1}{N_m}\\sum_{i\\in R_m}I(y_i\\neq k(m))=1-\\hat p_{mk(m)}$，其中$k(m)=\\arg\\max_k\\hat p_{mk}$。\n",
    "* 基尼指数：$\\sum_{k\\neq k'}\\hat p_{mk}\\hat p_{mk'}=\\sum_{k=1}^K\\hat p_{mk}(1-\\hat p_{mk})$\n",
    "* 交叉熵（cross-entropy，也叫作deviance）：$-\\sum_{k=1}^K\\hat p_{mk}\\ln \\hat p_{mk}$\n",
    "\n",
    "其中$\\hat p_{mk}$为节点$m$中类别$k$所占的比率为：$\\hat p_{mk}=\\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k)$\n",
    "\n",
    "生成分类树可以使用基尼指数或交叉熵。进行剪枝可以使用任意一种度量方法，但通常使用误分率。\n",
    "\n",
    "其他说明\n",
    "* 树模型的一大问题是其模型复杂度较高导致方差较大，容易过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Boosting\n",
    "* 直观理解：不断调整样本数据的权重然后进行训练，依次产生一系列弱分类器，最后集成这些弱分类器形成强分类器。\n",
    "* 讨论Boosting方法时，$Y\\in\\{-1,1\\}$。\n",
    "\n",
    "#### 2.5.1 AdaBoost算法\n",
    "\n",
    "1. 初始化数据权重$w_i=1/N$，$i=1,2,…,N$\n",
    "2. 从$m=1$到$M$：\n",
    "\n",
    "\t(a) 对权重为$w_i$的训练数据进行拟合得到分类器$G_m(x)$\n",
    "    \n",
    "\t(b) 计算：\n",
    "\n",
    "    $$err_m=\\frac{\\sum_{i=1}^Nw_iI(y_i\\neq G_m(x_i))}{\\sum_{i=1}^Nw_i}$$\n",
    "\n",
    "\t(c) 计算$\\alpha_m=\\ln\\frac{1-err_m}{err_m}$\n",
    "    \n",
    "\t(d) 更新权重：$w_i\\leftarrow w_i\\cdot exp⁡[\\alpha_m\\cdot I(y_i\\neq G_m(x_i))], i=1,2,…,N$\n",
    "3. 输出：$G(x)=sign[\\sum_{m=1}^M\\alpha_mG_m(x)]$\n",
    "\n",
    "AdaBoost算法等价于使用指数损失来求解前向逐步叠加模型（Foward Stagewise Additive Modeling）。\n",
    "\n",
    "证明：\n",
    "\n",
    "在第$m$步，前向逐步叠加模型对$f(x)$的更新公式为：$f_m(x)=f_{m-1}(x)+\\beta G(x)$，模型参数为$\\beta$和$G(x)$，其中$G(x)$为当前的分类器，$G(x)\\in \\{-1,1\\}$。\n",
    "\n",
    "指数损失：$L(y,f(x))=exp⁡(-yf(x))$，其中$y\\in\\{-1,1\\}$，通过最小化经验指数损失之和，求解前向逐步叠加模型中的参数为：\n",
    "\n",
    "\\begin{align}\n",
    "(\\beta_m,G_m)\n",
    "&=\\underset{\\beta,G}{\\arg\\min}\\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\\beta G(x_i))]\\\\\n",
    "&=\\underset{\\beta,G}{\\arg\\min}\\sum_{i=1}^{N}w_i^{(m)}exp[-\\beta y_iG(x_i)]\n",
    "\\end{align}\n",
    "\n",
    "其中$w_i^{(m)}=exp[-y_if_{m-1}(x_i)]$\n",
    "\n",
    "进一步转化得：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\beta_m,G_m)\n",
    "&=\\underset{\\beta,G}{\\arg\\min}\\{\\sum_{i=1}^{N}w_i^{(m)}e^{-\\beta}I(y_i=G(x_i))+\\sum_{i=1}^{N}w_i^{(m)}e^{\\beta}I(y_i\\neq G(x_i))\\}\\\\\n",
    "&=\\underset{\\beta,G}{\\arg\\min}\\{e^{-\\beta}\\sum_{i=1}^{N}w_i^{(m)}I(y_i=G(x_i))+e^{\\beta}\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))\\}\\\\\n",
    "&=\\underset{\\beta,G}{\\arg\\min}\\{e^{-\\beta}\\sum_{i=1}^{N}w_i^{(m)}+(e^{\\beta}-e^{-\\beta})\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))\\}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "所以，\n",
    "\n",
    "$$G_m=\\underset{G}{\\arg\\min}\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))$$\n",
    "\n",
    "这对应Adaboost算法中的2a。\n",
    "\n",
    "下面通过对上述损失进行求导来求解另一个参数$\\beta_m$。\n",
    "\n",
    "$$L(\\beta)=\\sum_{i=1}^Nw_i^{(m)}\\cdot[e^{-\\beta}+\\frac{(e^\\beta-e^{-\\beta})\\sum_{i=1}^Nw_i^{(m)}I(y_i\\neq G(x_i))}{\\sum_{i=1}^Nw_i^{(m)}}]$$\n",
    "\n",
    "设$err_m=\\frac{\\sum_{i=1}^Nw_i^{(m)}I(y_i\\neq G(x_i))}{\\sum_{i=1}^Nw_i^{(m)}}$（对应Adaboost算法中的2b），则$L(\\beta)$可化简为：\n",
    "\n",
    "$$L(\\beta)=\\sum_{i=1}^Nw_i^{(m)}\\cdot[e^{-\\beta}+err_m(e^\\beta-e^{-\\beta})]$$\n",
    "\n",
    "对$L(\\beta)$进行求导得：\n",
    "\n",
    "$$\\frac{\\partial L(\\beta)}{\\partial \\beta}=\\sum_{i=1}^Nw_i^{(m)}\\cdot[-e^{-\\beta}+err_m(e^\\beta+e^{-\\beta})]=0$$\n",
    "\n",
    "得：\n",
    "\n",
    "$$\\beta_m=\\frac{1}{2}\\ln\\frac{1-err_m}{err_m}$$\n",
    "\n",
    "此时有：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_i^{m+1}\n",
    "&=e^{-y_if_m(x_i)}\\\\\n",
    "&=e^{-y_i(f_{m-1}(x)+\\beta_m G_m(x))}\\\\\n",
    "&=w_i^{m}e^{-y_i\\beta_m G_m(x)}\\\\\n",
    "&=w_i^{m}e^{[2I(y_i\\neq G_m(x))-1]\\beta_m}\\\\\n",
    "&=w_i^{m}e^{2\\beta_mI(y_i\\neq G_m(x))}e^{-\\beta_m}\\\\\n",
    "&=w_i^{m}e^{\\alpha_mI(y_i\\neq G_m(x))}e^{-\\beta_m}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中$\\alpha_m=2\\beta_m=\\ln\\frac{1-err_m}{err_m}$，这一结果对应Adaboost算法中的2c和2d（$e^{-\\beta_m}$不会对数据间的相对权重产生改变因此不改变结果）。证毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 梯度Boosting（Gradient Boosting）\n",
    "\n",
    "Boosting树是指以树模型为基模型的前向逐步叠加模型。若使用指数损失，那么Boosting树的求解步骤等同于Adaboost算法，对每一棵基树的求解也与一般的树模型一致。但若为了模型的鲁棒性使用deviance作为损失函数，这时就没有对应的简明算法而需要使用数值方法：梯度Boosting。梯度Boosting可以理解成梯度下降法与Boosting树相结合的算法。\n",
    "\n",
    "一般来说，监督学习都是找到使得经验损失最小的$f$，这相当于把损失函数对$f$求导，取导数为0时的$f$。若无法得到分析解，还可以采用数值方法逼近分析解，其中一种数值方法就是梯度下降法：\n",
    "\n",
    "$$\\textbf{f}_m=\\textbf{f}_{m-1}-\\rho_m \\textbf{g}_m$$\n",
    "\n",
    "其中$\\textbf{g}_m$为梯度，$g_{im}=[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]_{f(x_i)=f_{m-1}(x_i)}$。对于任何可微分的损失函数来说这一梯度都很好计算。\n",
    "\n",
    "另外，Boosting树可表示为：\n",
    "\n",
    "$$f_m=f_{m-1}+T(x;\\Theta_m)$$\n",
    "\n",
    "梯度Boosting就是把梯度下降法与Boosting树相结合，通过最小二乘法对负梯度值进行拟合得到回归树$T$：\n",
    "\n",
    "$$\\tilde\\Theta_m=\\underset{\\Theta_m}{\\arg\\min}\\sum_{i=1}^N(-g_{im}-T(x_i;\\Theta_m))^2$$\n",
    "\n",
    "Boosting树的实际经验总结\n",
    "\n",
    "可调参数：\n",
    "\n",
    "* $M$，boosting迭代次数（树的个数）\n",
    "* $J$，单棵树叶节点的最多个数，限定所有树都相同。经验表明可取$4\\leq J\\leq 8$\n",
    "* $\\gamma$，收缩参数，降低每棵树在当前估计中所占的比重，控制boosting的学习速率，缓解过拟合\n",
    "\n",
    "常用方法是设定较小的收缩参数($\\gamma<0.1$)和较大的迭代次数$M$，通过提前停止学习的方式控制拟合度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 随机森林（Random Forest）\n",
    "随机森林：Bagging（bootstrap aggregation，基模型为树模型）+ 随机选择输入变量。随机森林可以进一步降低Bagging的方差。\n",
    "\n",
    "包外（Out of Bag，OOB）误差：随机森林模型的一大特色是可以使用包外误差来估计模型的预测性能。包外数据是指使用自助法（bootstrap）选择数据时一次也没有选择到的数据所组成的数据集。包外误差与交叉验证误差结果相似（另一种看法认为包外误差过于乐观）。\n",
    "\n",
    "当与输出相关联的输入变量个数较少、而随机选择输入变量的个数又太少时，随机森林的性能会很差，相反随机森林的性能会很稳定。\n",
    "\n",
    "随机森林不会导致过拟合。经验表明，随机森林中使用完全长成的树不会导致不良后果，而这样做的好处是可以减少一个需要调整的超参数。\n",
    "\n",
    "两个可调参数：\n",
    "* 树的个数\n",
    "* 输入变量个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 贝叶斯方法\n",
    "\n",
    "####  2.7.1 线性判别分析（Linear Discriminant Analysis，LDA）\n",
    "\n",
    "使用贝叶斯公式计算条件概率$P(Y=g|x)$：\n",
    "\n",
    "$$P⁡(Y=g│x)=\\frac{f(x|Y=g)P(Y=g)}{\\sum_{g=1}^Gf(x|Y=g)P(Y=g)}$$\n",
    "\n",
    "其中$f(x|Y=g)$为类别$g$的概率密度，$P(Y=g)$为类别$g$的先验概率。\n",
    "\n",
    "两个假设：\n",
    "* 假设类别$g$的概率密度为多元高斯分布：\n",
    "$f(x|Y=g)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma_g|^{1/2}}e^{-\\frac{1}{2}(x-\\mu_g)^T \\Sigma_g^{-1}(x-\\mu_g)}$。\n",
    "* 不同类别的协方差矩阵相同：$\\Sigma_g=\\Sigma$ $\\forall g$。\n",
    "\n",
    "对模型中的参数进行极大似然估计：\n",
    "\n",
    "* $\\hat P⁡(Y=g)=N_g/N$\n",
    "* $\\hat\\mu_g=\\Sigma_{g_i=g}x_i/N_g$\n",
    "* $\\hat\\Sigma=\\Sigma_{g=1}^G\\Sigma_{g_i=g}(x_i-\\hat\\mu_g)(x_i-\\hat\\mu_g)^T/(N-G)$\n",
    "\n",
    "二次判别分析（Quadratic Discriminant Analysis，QDA）：并不假设不同类别的$\\Sigma_k$相同。\n",
    "\n",
    "#### 2.7.2 朴素贝叶斯（Naive Bayes）\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=g|x)\n",
    "&=\\frac{P(x|Y=g)P(Y=g)}{\\sum_{g=1}^G P(x|Y=g)P(Y=g)}\\\\\n",
    "&=\\frac{\\prod_{j=1}^p P(x^j|Y=g)P(Y=g)}{\\sum_{g=1}^G \\prod_{j=1}^p P(x^j|Y=g)P(Y=g)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "步骤1为贝叶斯公式，步骤2基于以下假设：所有的特征变量在给定$Y$的条件下两两独立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 支持向量机（Support Vector Machine）\n",
    "\n",
    "支持向量机属于分类算法，但不使用基于概率与统计的求解框架，而是把分类问题转化为凸优化（Convex Optimization）问题进行求解。\n",
    "\n",
    "#### 2.8.1 最优分类超平面（Optimal Separating Hyperplanes）\n",
    "\n",
    "首先讨论线性可分这一简单情形，即用线性超平面就可以将不同类别的数据进行划分。最好的划分超平面使得所有点离超平面的距离最远。下面从这一点出发进行求解。\n",
    "\n",
    "划分超平面可表示为：\n",
    "\n",
    "$$\\beta_0+\\beta^T x=0$$\n",
    "\n",
    "令$y\\in\\{-1, 1\\}$。对于$y=1$的点，需要$\\beta_0+\\beta^T x>0$；对于$y=-1$的点，需要$\\beta_0+\\beta^T x<0$。\n",
    "\n",
    "由空间解析几何知，超平面外一点$x$到超平面的距离为：\n",
    "\n",
    "$$L=\\frac{|\\beta_0+\\beta^T x|}{||\\beta||}=\\frac{y(\\beta_0+\\beta^T x)}{||\\beta||}$$\n",
    "\n",
    "那么优化问题可表述为：\n",
    "\n",
    "$$\\underset{\\beta, \\beta_0}{\\max}M$$\n",
    "限定条件为：$$\\frac{y_i(\\beta_0+\\beta^T x_i)}{||\\beta||}\\geq M, i=1,...,N$$\n",
    "\n",
    "因为$\\beta_0$和$\\beta^T$可以同倍地放大缩小而并不改变求解结果，因此其绝对值大小不受限制，所以可以任意指定$||\\beta||=\\frac{1}{M}$，\n",
    "\n",
    "则这一优化问题转化为：\n",
    "\n",
    "$$\\underset{\\beta, \\beta_0}{\\min} \\frac{1}{2}||\\beta||^2$$\n",
    "限定条件为：$$1-y_i(\\beta_0+\\beta^T x_i)\\leq 0, i=1,...,N$$\n",
    "\n",
    "这一问题可用拉格朗日乘数法进行求解。\n",
    "\n",
    "#### 不等式约束条件的拉格朗日乘数法\n",
    "\n",
    "问题：\n",
    "\n",
    "求函数$z=f(x,y)$在条件$\\varphi(x,y)\\leq 0$下取得**极小值**的条件。\n",
    "\n",
    "方法：\n",
    "\n",
    "作拉格朗日函数：$L(x,y,\\lambda)=f(x,y)+\\lambda \\varphi(x,y)$，分两种情况讨论：\n",
    "1. 极小值在$\\varphi(x,y)=0$取得，则：\n",
    "    * $\\lambda \\varphi(x,y)=0$成立\n",
    "    * $\\nabla f+\\lambda\\nabla \\varphi=0$\n",
    "    * $\\nabla f$与$\\nabla \\varphi$反方向。$\\nabla \\varphi$指向边界朝外（边界上的值比边界内的值大），$\\nabla f$指向边界朝内（边界上取得极小值），所以$\\lambda >0$\n",
    "2. 极小值在$\\varphi(x,y)<0$取得，则为非约束求解问题：\n",
    "    * $\\lambda=0$，$\\lambda \\varphi(x,y)=0$成立\n",
    "    * $\\nabla f+\\lambda\\nabla \\varphi=0$成立\n",
    "\n",
    "由上述分析得方程组：\n",
    "\n",
    "$$\\nabla f+\\lambda\\nabla \\varphi=0$$\n",
    "$$\\lambda \\varphi(x,y)=0$$\n",
    "$$\\varphi(x,y)\\leq 0$$\n",
    "$$\\lambda\\geq 0$$\n",
    "\n",
    "后三个条件称为Karush-Kuhn-Tucker（KKT）条件。\n",
    "\n",
    "#### 使用拉格朗日乘数法求解最优分类超平面问题\n",
    "\n",
    "作拉格朗日函数：\n",
    "\n",
    "$$L_P=\\frac{1}{2}||\\beta||^2+\\sum_{i=1}^N \\alpha_i[1-y_i(\\beta_0+\\beta^T x_i)]$$\n",
    "\n",
    "对$\\beta$和$\\beta_0$求导可得：\n",
    "\n",
    "$$\\beta=\\sum_{i=1}^N\\alpha_i y_ix_i$$\n",
    "$$0=\\sum_{i=1}^N\\alpha_iy_i$$\n",
    "\n",
    "又由KKT条件得：\n",
    "\n",
    "$$\\alpha_i[1-y_i(\\beta_0+\\beta^T x_i)]=0,i=1,...,N$$\n",
    "\n",
    "联立以上N+2个方程可对问题进行求解。\n",
    "\n",
    "#### 对偶问题\n",
    "\n",
    "最优划分超平面问题还可以将原始问题转化为对偶问题求解。这样的转化可以为后面线性不可分的情形提供统一解决方法。\n",
    "\n",
    "主优化问题\n",
    "\n",
    "$$\\min_{(x,y)} f(x,y)$$\n",
    "\n",
    "限定条件为：\n",
    "\n",
    "$$\\varphi_i(x,y)\\leq 0,i=1,...,N$$\n",
    "\n",
    "作拉格朗日函数：\n",
    "\n",
    "$$L(x,y,\\lambda)=f(x,y)+\\sum_{i=1}^N\\lambda_i \\varphi_i(x,y)$$\n",
    "\n",
    "由KKT条件知：$\\lambda_i \\geq 0$。\n",
    "\n",
    "令$\\theta_P(x,y)=\\max_{\\lambda}L(x,y,\\lambda)$，则：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\theta_P(x,y) = \\left\\{\n",
    "   \\begin{array}\\\\\n",
    "     f(x,y) & (x,y)满足限定条件 \\\\\n",
    "     \\infty & 其他 \\\\\n",
    "   \\end{array}\n",
    "   \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$p^\\ast=\\min_{(x,y)} f(x,y)=\\min_{(x,y)}\\theta_P(x,y)=\\min_{(x,y)}\\max_{\\lambda}L(x,y,\\lambda)$$\n",
    "\n",
    "对偶优化问题\n",
    "\n",
    "令$\\theta_D(\\lambda)=\\min_{(x,y)}L(x,y,\\lambda)$，则：\n",
    "\n",
    "$$d^\\ast=\\max_{\\lambda}\\theta_D(\\lambda)=\\max_{\\lambda}\\min_{(x,y)}L(x,y,\\lambda)$$\n",
    "\n",
    "一般情况下：\n",
    "\n",
    "$$d^\\ast=\\max_{\\lambda}\\min_{(x,y)}L(x,y,\\lambda)\\leq\\min_{(x,y)}\\max_{\\lambda}L(x,y,\\lambda)=p^\\ast$$\n",
    "\n",
    "而当$f$和$\\varphi_i$为凸函数时，满足Slater条件，有$d^\\ast=p^\\ast$。所以可以通过求解对偶问题，得到原优化问题的解。\n",
    "\n",
    "#### 最优分类超平面问题的对偶问题\n",
    "\n",
    "原始问题的拉格朗日函数为：\n",
    "\n",
    "$$L_P=\\frac{1}{2}||\\beta||^2+\\sum_{i=1}^N \\alpha_i[1-y_i(\\beta_0+\\beta^T x_i)]$$\n",
    "\n",
    "又：\n",
    "\n",
    "$$\\beta=\\sum_{i=1}^N\\alpha_i y_ix_i$$\n",
    "$$0=\\sum_{i=1}^N\\alpha_iy_i$$\n",
    "\n",
    "带入到拉格朗日函数中得对偶问题为：\n",
    "\n",
    "$$\\max_\\alpha L_D = \\max_\\alpha \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{k=1}^N\\alpha_i\\alpha_ky_iy_kx_i^Tx_k$$\n",
    "\n",
    "限定条件为：\n",
    "\n",
    "$$\\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "$$\\alpha_i\\geq0, i=1,...,N$$\n",
    "\n",
    "通过求解这一对偶问题，可以得到原始优化问题的解。\n",
    "\n",
    "#### 2.8.2 允许软间隔的分类超平面\n",
    "\n",
    "前面讨论了线性可分的情况，下面讨论线性不可分的情况。线性不可分的求解问题表述为：\n",
    "\n",
    "$$\\underset{\\beta, \\beta_0}{\\min} \\frac{1}{2}||\\beta||^2$$\n",
    "\n",
    "限定条件为\n",
    "\n",
    "$$y_i(\\beta_0+\\beta^T x_i)\\leq 1-\\xi_i$$\n",
    "$$\\xi_i \\geq 0$$\n",
    "$$\\sum\\xi_i \\leq constant, i=1,...,N$$\n",
    "\n",
    "等价于：\n",
    "\n",
    "$$\\underset{\\beta, \\beta_0}{\\min} \\frac{1}{2}||\\beta||^2+C\\sum_{i=1}^N \\xi_i$$\n",
    "\n",
    "限定条件为\n",
    "\n",
    "$$y_i(\\beta_0+\\beta^T x_i)\\leq 1-\\xi_i$$\n",
    "$$\\xi_i \\geq 0$$\n",
    "\n",
    "拉格朗日函数为：\n",
    "\n",
    "$$L_P=\\frac{1}{2}||\\beta||^2+C\\sum_{i=1}^N \\xi_i+\\sum_{i=1}^N \\alpha_i[1-\\xi_i-y_i(\\beta_0+\\beta^T x_i)]-\\sum_{i=1}^N\\mu_i\\xi_i$$\n",
    "\n",
    "对$\\beta$、$\\beta_0$和$\\xi_i$求导得：\n",
    "\n",
    "$$\\beta=\\sum_{i=1}^N\\alpha_i y_ix_i$$\n",
    "$$0=\\sum_{i=1}^N\\alpha_iy_i$$\n",
    "$$\\alpha_i=C-\\mu_i, i=1,...,N$$\n",
    "\n",
    "带入拉格朗日函数中，得对偶问题为：\n",
    "\n",
    "$$\\max_\\alpha L_D = \\max_\\alpha\\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{k=1}^N\\alpha_i\\alpha_ky_iy_kx_i^Tx_k$$\n",
    "\n",
    "$$\\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "$$0\\leq\\alpha_i\\leq C, i=1,...,N$$\n",
    "\n",
    "#### 2.8.3 使用核函数的支持向量机\n",
    "\n",
    "核函数将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。\n",
    "\n",
    "$$L_D = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{k=1}^N\\alpha_i\\alpha_ky_iy_k\\langle h(x_i),h(x_k)\\rangle= \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{k=1}^N\\alpha_i\\alpha_ky_iy_kK(x,x')$$\n",
    "\n",
    "常用核函数：\n",
    "\n",
    "* d阶多项式：$K(x,x')=(1+\\langle x,x'\\rangle)^d$\n",
    "* Radial basis：$K(x,x')=\\exp(-\\gamma||x-x'||^2)$\n",
    "* 神经网络：$K(x,x')=\\tanh(\\kappa_1\\langle x,x'\\rangle+\\kappa_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考资料\n",
    "\n",
    "[1] Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\n",
    "\n",
    "[2] 周志华（2016）机器学习，清华大学出版社\n",
    "\n",
    "[3] 线上课程：Harvard CS109: Data Science, https://cs109.github.io/2015/\n",
    "\n",
    "[4] 线上课程：Machine Learning, Coursea, https://www.coursera.org/learn/machine-learning\n",
    "\n",
    "[5] 线上课程：神经网络和深度学习等，网易云课堂，https://mooc.study.163.com/university/deeplearning_ai#/c\n",
    "\n",
    "[6] 线上课程：Stanford CS229: Machine Learning, http://cs229.stanford.edu/\n",
    "\n",
    "[7] 线上课程：机器学习（进阶），优达学城，https://cn.udacity.com/course/machine-learning-engineer-advanced-nanodegree--nd009-cn-advanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
