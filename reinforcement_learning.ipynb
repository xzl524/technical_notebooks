{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习\n",
    "\n",
    "### 主要内容\n",
    "\n",
    "* 1 强化学习综述\n",
    "    * 1.1 问题特点\n",
    "    * 1.2 示例：$n$-摇臂赌博机问题\n",
    "    * 1.3 Bellman等式\n",
    "* 2 强化学习算法\n",
    "    * 2.1 动态规划\n",
    "    * 2.2 蒙特卡罗方法\n",
    "    * 2.3 时序差分学习\n",
    "    * 2.4 资格迹\n",
    "* 3 其他内容\n",
    "* 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 强化学习综述\n",
    "\n",
    "#### 1.1 问题特点\n",
    "* 延迟奖励：以最终目标为导向\n",
    "    * 处理方法\n",
    "        * 选择期望累计奖励最多的策略\n",
    "    * 数学表达\n",
    "        * 累积奖励\n",
    "\n",
    "            $$R_t=\\sum_{k=0}^T\\gamma^kr_{t+k+1}$$\n",
    "\n",
    "        * 某一策略的状态价值函数\n",
    "\n",
    "            $$V^\\pi(s)=E_\\pi(R_t|s_t=s)=E_\\pi(\\sum_{k=0}^\\infty\\gamma^kr_{t+k+1}|s_t=s)$$\n",
    "\n",
    "        * 某一策略的动作价值函数\n",
    "\n",
    "            $$Q^\\pi(s,a)=E_\\pi(R_t|s_t=s,a_t=a)=E_\\pi(\\sum_{k=0}^\\infty\\gamma^kr_{t+k+1}|s_t=s,a_t=a)$$\n",
    "\n",
    "            这两者的关系为：\n",
    "\n",
    "            $$V^\\pi(s)=\\sum_a\\pi(s,a)Q^\\pi(s,a)$$\n",
    "\n",
    "        * 最优策略\n",
    "            $$\\pi^\\ast=\\arg\\max_\\pi V^\\pi(s)=\\arg\\max_\\pi Q^\\pi(s,a)$$\n",
    "* 试错学习：与环境交互\n",
    "    * 持续地进行以下循环操作：估计价值函数（探索）$\\Leftrightarrow$ 基于估计做出选择（利用）\n",
    "\n",
    "#### 1.2 示例：$n$-摇臂赌博机问题\n",
    "\n",
    "这是最简化的强化学习问题设定：\n",
    "\n",
    "* 面对一个摇臂机（只有一个状态），需要选出期望奖励最多的摇臂（选择最优的动作）\n",
    "\n",
    "解决这一问题需要循环地执行以下两步操作：\n",
    "1. 估计动作的价值：估计各个摇臂的期望奖励\n",
    "\n",
    "    $$Q_t(a)=(r_1+r_2+...+r_{k_a})/k_a$$\n",
    "\n",
    "2. 选择动作：基于对价值的估计来选择摇臂\n",
    "    * 贪心选择法：只选择当前估价最高的动作\n",
    "    * $\\varepsilon$-贪心选择法：以$\\varepsilon$的概率探索新的动作，$1-\\varepsilon$的概率选择当前估价最高的动作\n",
    "    * Softmax选择法：选择各个动作的概率为$p(a)=e^{Q_t(a)/\\tau}/\\sum_{b=1}^{n}e^{Q_t(b)/\\tau}$\n",
    "    \n",
    "#### 1.3 Bellman等式\n",
    "\n",
    "#### 状态价值函数的Bellman等式\n",
    "\n",
    "$$V^\\pi(s)=\\sum_a\\pi(s,a)\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V^\\pi(s^{\\prime})]$$\n",
    "\n",
    "\n",
    "其中：\n",
    "* $P_{ss^{\\prime}}^a$为状态转移概率：$P_{ss^{\\prime}}^a=P(s_{t+1}=s^{\\prime}|s_t=s,a_t=a)$\n",
    "* $R_{ss^{\\prime}}^a$为即刻奖励期望：$R_{ss^{\\prime}}^a=E(r_{t+1}|s_t=s,a_t=a,s_{t+1}=s^{\\prime})$\n",
    "\n",
    "同时假设这是马尔可夫决策过程（Markov Decision Process），满足马尔可夫性质：\n",
    "\n",
    "$$P(s_{t+1}=s^{\\prime},r_{t+1}=r|s_t,a_t,r_t,s_{t-1},a_{t-1}...)=P(s_{t+1}=s^{\\prime},r_{t+1}=r|s_t,a_t)$$\n",
    "\n",
    "证明：\n",
    "\n",
    "$$V^{\\pi}(s)=E_{\\pi}(R_t|s_t=s)=\\sum_a\\pi(s,a)\\sum_{s^{\\prime}}P_{ss^{\\prime}}^aE(R_t|s_t=s,a_t=a,s_{t+1}=s^{\\prime})$$\n",
    "\n",
    "\\begin{align}\n",
    "E(R_t|s_t=s,a_t=a,s_{t+1}=s^{\\prime})\n",
    "&=E(\\sum_{k=0}^{\\infty}\\gamma^kr_{t+k+1}|s_t=s,a_t=a,s_{t+1}=s^{\\prime})\\\\\n",
    "&=E(r_{t+1}+\\gamma\\sum_{k=0}^{\\infty}\\gamma^kr_{t+k+2}|s_t=s,a_t=a,s_{t+1}=s^{\\prime})\\\\\n",
    "&=R_{ss^{\\prime}}^a+\\gamma E(\\sum_{k=0}^{\\infty}\\gamma^kr_{t+1+k+1}|s_t=s,s_{t+1}=s^{\\prime},a_t=a)\\\\\n",
    "&=R_{ss^{\\prime}}^a+\\gamma E(\\sum_{k=0}^{\\infty}\\gamma^kr_{t+1+k+1}|s_{t+1}=s^{\\prime})\\\\\n",
    "&=R_{ss^{\\prime}}^a+\\gamma E_{\\pi}(\\sum_{k=0}^{\\infty}\\gamma^kr_{t+1+k+1}|s_{t+1}=s^{\\prime})\\\\\n",
    "&=R_{ss^{\\prime}}^a+\\gamma E_{\\pi}(R_{t+1}|s_{t+1}=s^{\\prime})\\\\\n",
    "&=R_{ss^{\\prime}}^a+\\gamma V^{\\pi}(s^{\\prime})\n",
    "\\end{align}\n",
    "\n",
    "所以\n",
    "\n",
    "$$V^{\\pi}(s)=\\sum_a\\pi(s,a)\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V^{\\pi}(s^{\\prime})]$$\n",
    "\n",
    "#### 动作价值函数的Bellman等式\n",
    "\n",
    "$$Q^{\\pi}(s,a)=\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma\\sum_{a^{\\prime}}\\pi(s^{\\prime},a^{\\prime})Q^{\\pi}(s^{\\prime},a^{\\prime})]$$\n",
    "\n",
    "证明：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a) &\n",
    "=E_{\\pi}(R_t|s_t=s,a_t=a)\\\\\n",
    " & =\\sum_{s^{\\prime}}P_{ss^{\\prime}}^aE(R_t|s_t=s,a_t=a,s_{t+1}=s^{\\prime})\\\\\n",
    " & =\\sum_{s^{\\prime}}P_{ss^{\\prime}}^aE(r_{t+1}+\\gamma\\sum_{k=0}^{\\infty}\\gamma^kr_{t+k+2}|s_t=s,a_t=a,s_{t+1}=s^{\\prime})\\\\\n",
    " & =\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a(R_{ss^{\\prime}}^a+\\gamma E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kr_{t+1+k+1}|s_{t+1}=s^{\\prime}])\\\\\n",
    " & =\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V^{\\pi}(s^{\\prime})]\\\\\n",
    " & =\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma\\sum_{a^{\\prime}}\\pi(s^{\\prime},a^{\\prime})Q^{\\pi}(s^{\\prime},a^{\\prime})]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 最优Bellman等式\n",
    "\n",
    "$$\n",
    "V^{\\ast}(s)=\\max_{a\\in A(s)}Q^{{\\pi}^{\\ast}}(s,a)=\\max_a\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V^{\\ast}(s^{\\prime})]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^{\\ast}(s,a)=\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma\\max_{a^{\\prime}}Q^{\\ast}(s^{\\prime},a^{\\prime})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 强化学习算法\n",
    "\n",
    "#### 2.1 动态规划（Dynamic Programming）\n",
    "\n",
    "动态规划为有模型算法，意思是状态转移概率和即刻奖励期望这些与环境相关的值已知。\n",
    "\n",
    "* 策略迭代（Policy Iteration）\n",
    "    * 策略评价：利用Bellman等式，通过迭代的方式计算某一策略的状态价值函数\n",
    "\n",
    "    $$V_{k+1}(s)=\\sum_a\\pi(s,a)\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V_k(s^{\\prime})]$$\n",
    "\n",
    "    * 策略改进\n",
    "\n",
    "    $$\\pi^{\\prime}(s)=\\arg\\max_a\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V^{\\pi}(s^{\\prime})]$$\n",
    "\n",
    "    * 循环执行策略评价和策略改进这两个步骤，以至最优。\n",
    "    \n",
    "* 价值迭代（Value Iteration）：将策略评价和策略改进合并为一体\n",
    "\n",
    "$$V_{k+1}(s)=\\max_a\\sum_{s^{\\prime}}P_{ss^{\\prime}}^a[R_{ss^{\\prime}}^a+\\gamma V_k(s^{\\prime})]$$\n",
    "\n",
    "    价值迭代也可以看做是把最优Bellman等式变成递推公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 蒙特卡罗方法（Monte Carlo Methods）\n",
    "\n",
    "蒙特卡罗方法为无模型算法，意味着状态转移概率和即刻奖励期望这些与环境相关的值未知。蒙特卡罗方法对以下状态价值函数表达式进行采样估计：\n",
    "\n",
    "$$V(s)=E_{\\pi}(R_t|s_t=s)$$\n",
    "\n",
    "估计方法\n",
    "* 采用常数步长的递增表达式为：\n",
    "\n",
    "$$V(s_t)\\leftarrow V(s_t)+\\alpha[R_t-V(s_t)]$$\n",
    "\n",
    "* 蒙特卡罗方法对价值估计的更新是在完成一个采样轨迹（episode）之后。\n",
    "* 因为无模型算法中状态转移概率和即刻奖励期望未知，因此仅对$V^{\\pi}(s)$进行估计是不够的，最主要的还是对$Q^{\\pi}(s,a)$进行估计。\n",
    "\n",
    "#### 蒙特卡罗控制\n",
    "* 估计某一策略的价值函数\n",
    "* 选择策略：\n",
    "    * 同策略蒙特卡罗算法：$\\epsilon$-贪心策略\n",
    "    * 异策略蒙特卡罗算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 时序差分学习（Temporal-Difference Learning）\n",
    "\n",
    "时序差分（TD）学习为无模型算法，意味着状态转移概率和即刻奖励期望这些与环境相关的值未知。TD(0)对以下状态价值函数表达式进行采样估计：\n",
    "\n",
    "$$V^{\\pi}(s)=E_{\\pi}\\{r_{t+1}+\\gamma\\sum_{k=0}^{\\infty}\\gamma^kr_{t+k+2}|s_t=s\\}=E_{\\pi}\\{r_{t+1}+\\gamma V^{\\pi}(s_{t+1})|s_t=s\\}$$\n",
    "\n",
    "估计方法\n",
    "\n",
    "* 采用常数步长的递增表达式为：\n",
    "\n",
    "$$V(s_t)\\leftarrow V(s_t)+\\alpha[r_{t+1}+\\gamma V(s_{t+1})-V(s_t)]$$\n",
    "\n",
    "* TD学习对价值估计的更新是在下一个时间节点。\n",
    "* 与蒙特卡罗方法相同，时序差分学习也需要对$Q^{\\pi}(s,a)$进行估计。\n",
    "\n",
    "#### Sarsa：同策略TD控制\n",
    "\n",
    "$$Q(s_t,a_t)\\leftarrow Q(s_t,a_t )+\\alpha[r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$$\n",
    "\n",
    "#### Q-学习：异策略TD控制\n",
    "\n",
    "$$Q(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha[r_{t+1}+\\gamma\\max_{a}Q(s_{t+1},a)-Q(s_t,a_t)]$$\n",
    "\n",
    "Q-学习也可以看做是对最优Bellman等式中的$Q^{\\ast}(s,a)$的直接估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 资格迹（Eligibility Traces）\n",
    "\n",
    "两种理解资格迹的等价视角\n",
    "\n",
    "* 前向视角（forward view）：用来理解资格迹算法到底在计算什么，从理论上把TD学习和蒙特卡罗方法联系起来。\n",
    "* 反向视角（backward view）：实际使用的资格迹算法\n",
    "\n",
    "#### $TD(\\lambda)$的前向视角\n",
    "\n",
    "$n$步$TD$预测\n",
    "\n",
    "$$R_t^{(n)}=r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+3}+...+\\gamma^{n-1} r_{t+n}+\\gamma^n V_t(s_{t+n})$$\n",
    "\n",
    "$n=1$时，为TD(0)；$n=\\infty$时，为蒙特卡罗方法。\n",
    "\n",
    "$TD(\\lambda)$：对$n$步$TD$预测$R_t^{(n)}$进行加权平均\n",
    "\n",
    "$$R_t^\\lambda =(1-\\lambda)\\sum_{n=1}^\\infty\\lambda^{n-1}R_t^{(n)}=(1-\\lambda)\\sum_{n=1}^{T-t-1}\\lambda^{n-1}R_t^{(n)}+\\lambda^{T-t-1}R_t$$\n",
    "\n",
    "$$\\Delta V_t(s_t)=\\alpha[R_t^\\lambda-V_t(s_t)]$$\n",
    "\n",
    "#### $TD(\\lambda)$的反向视角\n",
    "\n",
    "资格迹：\n",
    "\n",
    "\\begin{equation}\n",
    "   e_t(s) = \\left\\{\n",
    "   \\begin{array}\n",
    "     \\gamma\\lambda e_{t-1}(s) & s \\neq s_t \\\\\n",
    "     \\gamma\\lambda e_{t-1}(s)+1 & s=s_t \\\\\n",
    "   \\end{array}\n",
    "   \\right.\n",
    "\\end{equation}\n",
    "\n",
    "$$\\delta_t=r_{t+1}+\\gamma V_t(s_{t+1})-V_t(s_t)$$\n",
    "\n",
    "$$\\Delta V_t(s_t)=\\alpha\\delta_te_t(s)$$\n",
    "\n",
    "两种视角等价：\n",
    "\n",
    "$$\\sum_{t=0}^{T-1}\\Delta V_t^{TD}(s)=\\sum_{t=0}^{T-1}\\Delta V_t^\\lambda(s_t)I_{ss_t}$$\n",
    "\n",
    "证明：\n",
    "\n",
    "首先用数学归纳法证明资格迹可以表示为：\n",
    "\n",
    "$$e_t(s)=\\sum_{k=0}^t(\\gamma\\lambda)^{t-k}I_{ss_k}$$\n",
    "\n",
    "当$t=0$时，由资格迹的定义可知$e_t(s)=I_{ss_0}$，满足上述等式。\n",
    "\n",
    "当$t=1$时，由资格迹的定义可知$e_t(s)=\\gamma\\lambda I_{ss_0}I_{s\\neq s_1}+(\\gamma\\lambda I_{ss_0}+1)I_{ss1}=\\gamma\\lambda I_{ss_0} (I_{ss_1}+ I_{s\\neq s_1})+I_{ss1}=\\gamma\\lambda I_{ss_0}+I_{ss1}$，满足上述等式。\n",
    "\n",
    "假设对任意$t\\geqslant1$，$e_t(s)=\\sum_{k=0}^t(\\gamma\\lambda)^{t-k}I_{ss_k}$。那么对于$e_{t+1}(s)$，由资格迹的定义可知，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "e_{t+1}(s)\n",
    "&=\\gamma\\lambda e_t(s)I_{s\\neq s_{t+1}}+(\\gamma\\lambda e_t(s)+1)I_{ss_{t+1}}\\\\\n",
    "&=\\gamma\\lambda e_t(s)(I_{s\\neq s_{t+1}}+I_{ss_{t+1}})+I_{ss_{t+1}}\\\\\n",
    "&=\\gamma\\lambda e_t(s)+I_{ss_{t+1}}\\\\\n",
    "&=\\gamma\\lambda \\sum_{k=0}^t(\\gamma\\lambda)^{t-k}I_{ss_k}+I_{ss_{t+1}}\\\\\n",
    "&=\\sum_{k=0}^t(\\gamma\\lambda)^{t+1-k}I_{ss_k}+I_{ss_{t+1}}\\\\\n",
    "&=\\sum_{k=0}^{t+1}(\\gamma\\lambda)^{t+1-k}I_{ss_k}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "所以对于任意$t\\geqslant0$，均有$e_t(s)=\\sum_{k=0}^t(\\gamma\\lambda)^{t-k}I_{ss_k}$。\n",
    "\n",
    "对于双重求和，有：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{k=0}^{T-1}\\sum_{t=0}^k\n",
    "&=(\\sum_{k=t}^{T-1}+\\sum_{k=0}^{t-1})(\\sum_{t=0}^{T-1}-\\sum_{t=k+1}^{T-1})\\\\\n",
    "&=\\sum_{k=t}^{T-1}\\sum_{t=0}^{T-1}-\\sum_{k=t}^{T-1}\\sum_{t=k+1}^{T-1}+\\sum_{k=0}^{t-1}\\sum_{t=0}^k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中后面两项都为零。以第二项为例。由求和上下限的定义可得，$k\\geqslant t$及$t\\geqslant k+1$，而这两个不等式是矛盾的，所以此求和为零。最后一项以此类推。\n",
    "\n",
    "所以\n",
    "\n",
    "$$\\sum_{k=0}^{T-1}\\sum_{t=0}^k=\\sum_{k=t}^{T-1}\\sum_{t=0}^{T-1}=\\sum_{t=0}^{T-1}\\sum_{k=t}^{T-1}$$\n",
    "\n",
    "所以\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{t=0}^{T-1}\\Delta V_t^{TD}(s)\n",
    "&=\\sum_{t=0}^{T-1}\\alpha\\delta_t\\sum_{k=0}^t(\\gamma\\lambda)^{t-k}I_{ss_k}\\\\\n",
    "&=\\sum_{k=0}^{T-1}\\alpha\\sum_{t=0}^k(\\gamma\\lambda)^{k-t}I_{ss_t}\\delta_k (交换t和k)\\\\\n",
    "&=\\sum_{t=0}^{T-1}\\alpha\\sum_{k=t}^{T-1}(\\gamma\\lambda)^{k-t}I_{ss_t}\\delta_k\\\\\n",
    "&=\\sum_{t=0}^{T-1}\\alpha I_{ss_t}\\sum_{k=t}^{T-1}(\\gamma\\lambda)^{k-t}\\delta_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "后面证明参照$^{[1]}$。\n",
    "\n",
    "#### Sarsa($\\lambda$)\n",
    "\n",
    "$$Q_{t+1}(s,a)=Q_t(s,a)+\\alpha\\delta_te_t(s,a)$$\n",
    "\n",
    "$$\\delta_t=r_{t+1}+\\gamma Q_t(s_{t+1},a_{t+1})-Q_t(s_t,a_t)$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   e_t(s,a) = \\left\\{\n",
    "   \\begin{array}\\\\\n",
    "     \\gamma\\lambda e_{t-1}(s,a)+1 & s=s_t,a=a_t \\\\\n",
    "     \\gamma\\lambda e_{t-1}(s,a) & 其他 \\\\\n",
    "   \\end{array}\n",
    "   \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Watkin’s Q($\\lambda$)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   e_t(s,a)=I_{ss_t}\\cdot I_{aa_t}+ \\left\\{\n",
    "   \\begin{array}\\\\\n",
    "     \\gamma\\lambda e_{t-1}(s,a) & 当 Q_{t-1}(s_t,a_t)=\\max_aQ_{t-1}(s_t,a) \\\\\n",
    "     0 & 其他 \\\\\n",
    "   \\end{array}\n",
    "   \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$Q_{t+1}(s,a)=Q_t(s,a)+\\alpha\\delta_te_t(s,a)$$\n",
    "\n",
    "$$\\delta_t=r_{t+1}+\\gamma\\max_{a^{\\prime}}Q_t(s_{t+1},a^{\\prime})-Q_t(s_t,a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 其他内容\n",
    "\n",
    "#### 3.1 泛化与函数近似\n",
    "泛化\n",
    "* 从有限的状态$\\mapsto$价值数据（$s\\mapsto v$）得到整个状态空间的价值函数。\n",
    "\n",
    "泛化的方法是函数近似，也就是监督学习。强化学习因为训练集实时更新，适用随机梯度下降法（Stochastic Gradient Descent, SGD），即在线学习的方式更新泛化价值函数的参数$\\overrightarrow{\\rm \\theta_t}$。\n",
    "\n",
    "训练集数据$s\\mapsto v$由动态规划（DP）、蒙特卡罗（MC）、时序差分或者资格迹等方法得到。价值函数可选用神经网络模型或者线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考资料\n",
    "\n",
    "[1] Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
